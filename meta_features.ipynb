{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnynzpondcvY",
        "outputId": "44745350-70e7-49a6-fbbb-761799c222d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting orkg\n",
            "  Downloading orkg-0.21.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated<2.0.0,>=1.2.14 (from orkg)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting Faker<20.0.0,>=19.1.0 (from orkg)\n",
            "  Downloading Faker-19.13.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Inflector<4.0.0,>=3.1.0 (from orkg)\n",
            "  Downloading Inflector-3.1.1-py3-none-any.whl (12 kB)\n",
            "Collecting cardinality<0.2.0,>=0.1.1 (from orkg)\n",
            "  Downloading cardinality-0.1.1.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hammock<0.3.0,>=0.2.4 (from orkg)\n",
            "  Downloading hammock-0.2.4.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from orkg)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx<4.0,>=3.1 in /usr/local/lib/python3.10/dist-packages (from orkg) (3.2.1)\n",
            "Collecting pandas<3.0.0,>=2.0.1 (from orkg)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from orkg) (2.6.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from orkg) (2.31.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from orkg) (4.66.2)\n",
            "Collecting undecorated<0.4.0,>=0.3.0 (from orkg)\n",
            "  Downloading undecorated-0.3.0-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated<2.0.0,>=1.2.14->orkg) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker<20.0.0,>=19.1.0->orkg) (2.8.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.1->orkg) (1.25.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.1->orkg) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=2.0.1->orkg)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->orkg) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->orkg) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->orkg) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->orkg) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker<20.0.0,>=19.1.0->orkg) (1.16.0)\n",
            "Building wheels for collected packages: cardinality, hammock\n",
            "  Building wheel for cardinality (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cardinality: filename=cardinality-0.1.1-py3-none-any.whl size=2587 sha256=af49b3c71f6c704d0ba76f57ddf17a4514cb31e39b5db5ab0b1eae27e6cd0a5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/19/d1/2665c004b583a7d1880fa59055a3e462d6e35841a01b57010b\n",
            "  Building wheel for hammock (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hammock: filename=hammock-0.2.4-py3-none-any.whl size=2406 sha256=d6f337723a4d7c4024de9b18c0a614f3a16c6e2376e8312b0887b1d87cfb48dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/55/5b/e1e1e4366a23623af753c68c926db14ea6e58c0f0386511cfa\n",
            "Successfully built cardinality hammock\n",
            "Installing collected packages: undecorated, Inflector, cardinality, tzdata, loguru, Deprecated, pandas, hammock, Faker, orkg\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.25.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 Faker-19.13.0 Inflector-3.1.1 cardinality-0.1.1 hammock-0.2.4 loguru-0.7.2 orkg-0.21.0 pandas-2.2.1 tzdata-2024.1 undecorated-0.3.0\n",
            "Collecting openml\n",
            "  Downloading openml-0.14.2.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting liac-arff>=2.4.0 (from openml)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openml) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from openml) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from openml) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from openml) (2.2.1)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from openml) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from openml) (1.25.2)\n",
            "Collecting minio (from openml)\n",
            "  Downloading minio-7.2.5-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openml) (14.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openml) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openml) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->openml) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml) (3.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from minio->openml) (2024.2.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from minio->openml) (2.0.7)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from minio->openml) (23.1.0)\n",
            "Collecting pycryptodome (from minio->openml)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from minio->openml) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openml) (3.6)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->minio->openml) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.21)\n",
            "Building wheels for collected packages: openml, liac-arff\n",
            "  Building wheel for openml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.14.2-py3-none-any.whl size=158699 sha256=d5568a38d0ce257831833c8a4807a853e88901d90b205520bdca3d60ab5ea2e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/4e/af/5e721761d86375dbca82e63cc2470019e97815bc39f11451ea\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11716 sha256=f58346072fd7b160a15b3c0a748f814ff8db51de6135d8491e31c519388bdb5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/2a/9c/3895d9617f8f49a0883ba686326d598e78a1c2f54fe3cae86d\n",
            "Successfully built openml liac-arff\n",
            "Installing collected packages: xmltodict, pycryptodome, liac-arff, minio, openml\n",
            "Successfully installed liac-arff-2.5.0 minio-7.2.5 openml-0.14.2 pycryptodome-3.20.0 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install orkg\n",
        "!pip install openml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t29syc1kaY8"
      },
      "source": [
        "# **Define variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RstC9NAOvFay"
      },
      "outputs": [],
      "source": [
        "custom_classID = 'C72277' # You need to edit it.\n",
        "dataset_name_label = 'Class Test3'  # You need to edit it.\n",
        "dataset_name='ilpd'\n",
        "hasFeatures_predicate_id = 'P114013'\n",
        "feature_predicates = {\n",
        "    'Number of instances': 'P39021',\n",
        "    'Number of features': 'P72007',\n",
        "    'Number of classes': 'P52036',\n",
        "    'Standard Deviation Ratio': 'P114000',\n",
        "    'Class Entropy': 'P114002',\n",
        "    'Normal Entropy': 'P114003',\n",
        "    'hasFeature': 'P114013'\n",
        "}\n",
        "\n",
        "feature_predicates_2 = {\n",
        "    'Skewness': 'P59063',\n",
        "    'Kurtosis': 'P59064',\n",
        "    'Min values': 'P44107',\n",
        "    'Max values': 'P44108',\n",
        "    'Mean values': 'P67072',\n",
        "    'Median values': 'P57006',\n",
        "}\n",
        "possible_targets = ['Class', 'class', 'Defects', 'c', 'Author', 'band_type', 'defects', 'Prevention', 'problems', 'binaryClass', 'y', 'character', 'attribute_21', 'target', 'Result', 'Phase', 'result']\n",
        "\n",
        "automl_datasets = ['kr-vs-kp', 'letter', 'balance-scale', 'mfeat-factors', 'mfeat-fourier', 'breast-w',\n",
        "                   'mfeat-karhunen', 'mfeat-morphological', 'mfeat-zernike', 'optdigits', 'credit-approval',\n",
        "                   'credit-g', 'pendigits', 'diabetes', 'sick', 'spambase', 'splice', 'tic-tac-toe', 'vehicle',\n",
        "                   'electricity', 'satimage',  'isolet', 'vowel', 'analcatdata_authorship',\n",
        "                   'analcatdata_dmft', 'mnist_784', 'pc4', 'pc3', 'jm1', 'kc2', 'kc1', 'pc1', 'bank-marketing',\n",
        "                   'banknote-authentication', 'blood-transfusion-service-center', 'cnae-9', 'first-order-theorem-proving',\n",
        "                   'har', 'ilpd', 'madelon', 'nomao', 'ozone-level-8hr', 'phoneme', 'qsar-biodeg', 'wall-robot-navigation',\n",
        "                   'semeion', 'wdbc', 'adult', 'Bioresponse', 'PhishingWebsites', 'GesturePhaseSegmentationProcessed',\n",
        "                   'cylinder-bands', 'dresses-sales', 'numerai28.6', 'texture', 'dna', 'churn',\n",
        "                   'Devnagari-Script', 'CIFAR_10', 'MiceProtein', 'car', 'Internet-Advertisements', 'mfeat-pixel',\n",
        "                   'steel-plates-fault', 'wilt', 'segment', 'climate-model-simulation-crashes', 'Fashion-MNIST',\n",
        "                   'jungle_chess_2pcs_raw_endgame_complete', 'JapaneseVowels']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hUKM1cXdoAu_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "email = userdata.get('email')\n",
        "password = userdata.get('password')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "85cw2D6WoJsY"
      },
      "outputs": [],
      "source": [
        "from orkg import ORKG, Hosts\n",
        "\n",
        "orkg = ORKG(host=Hosts.SANDBOX, creds=(email, password))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Ugp79QkRHL"
      },
      "source": [
        "# **Create the Class with the CustomID**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVkRJDGMwW30",
        "outputId": "986f73ca-5b64-42e3-cff7-4701b0e0d2cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'C72277',\n",
              " 'label': 'Class Test3 Test',\n",
              " 'uri': None,\n",
              " 'description': None,\n",
              " 'created_at': '2024-03-19T20:59:45.215190356+01:00',\n",
              " 'created_by': 'ba7b42b0-dbeb-41fb-99d5-cb6b4140660e',\n",
              " 'modifiable': True,\n",
              " '_class': 'class'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "classes = orkg.classes.add(id=custom_classID, label=f'{dataset_name_label} Test').content\n",
        "classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSJdXOVwkC2_"
      },
      "source": [
        "# **Extract the Meta-Features from the Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VkYyb5dke49"
      },
      "source": [
        "# **Adding meta features that having single value to ORKG resources**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UfR0r0Dejv2r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from scipy.stats import entropy\n",
        "import time\n",
        "def extract_meta_features(dataset_name):\n",
        "    # Fetch the dataset from OpenML\n",
        "    dataset = datasets.fetch_openml(dataset_name, parser='auto', as_frame=True)\n",
        "\n",
        "    # Identify the target variable\n",
        "    possible_targets = ['Class', 'class', 'Defects', 'c', 'Author', 'band_type', 'defects', 'Prevention', 'problems', 'binaryClass', 'y', 'character', 'attribute_21', 'target', 'Result', 'Phase', 'result']\n",
        "    target = next((t for t in possible_targets if t in dataset.frame.columns), None)\n",
        "    if target is None:\n",
        "        raise ValueError(\"Target variable not found in the dataset.\")\n",
        "\n",
        "    # Extract meta-features\n",
        "    num_instances = dataset.frame.shape[0]\n",
        "    num_features = dataset.frame.shape[1]\n",
        "    num_classes = len(dataset.frame[target].unique())\n",
        "\n",
        "    num_instances = int(num_instances * 1.1) # Adding 10% to num_instances for testen\n",
        "    num_features = int(num_features * 1.1)\n",
        "    num_classes = int (num_classes * 1.1)\n",
        "    meta_features = {\n",
        "        'Number of instances': num_instances,\n",
        "        'Number of features': num_features,\n",
        "        'Number of classes': num_classes,\n",
        "        'Standard Deviation Ratio': None,\n",
        "        'Class Entropy': None,\n",
        "        'Normal Entropy': None,\n",
        "        'hasFeatures': {}\n",
        "    }\n",
        "    time.sleep(1)\n",
        "    for feature in dataset.frame.select_dtypes(include=np.number).columns:\n",
        "        meta_features['hasFeatures'][feature] = {\n",
        "            'Skewness': round(dataset.frame[feature].skew(), 2),\n",
        "            'Kurtosis': round(dataset.frame[feature].kurt(), 2),\n",
        "            'Min values': round(dataset.frame[feature].min(), 2),\n",
        "            'Max values': round(dataset.frame[feature].max(), 2),\n",
        "            'Mean values': round(dataset.frame[feature].mean(), 2),\n",
        "            'Median values': round(dataset.frame[feature].median(), 2)\n",
        "        }\n",
        "\n",
        "    # Calculate other meta-features\n",
        "    sd_ratio = round((dataset.frame.std(numeric_only=True) / dataset.frame.mean(numeric_only=True)).mean(), 2)\n",
        "    class_entropy = round(entropy(dataset.frame[target].value_counts(normalize=True), base=2), 2)\n",
        "    normal_entropy = round(class_entropy / np.log2(len(dataset.frame[target].unique())), 2)\n",
        "    meta_features['Standard Deviation Ratio'] = sd_ratio\n",
        "    meta_features['Class Entropy'] = class_entropy.item()\n",
        "    meta_features['Normal Entropy'] = normal_entropy\n",
        "\n",
        "    return meta_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process the Meta Features**"
      ],
      "metadata": {
        "id": "_MLV0QF-SkPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AqigYxBwH-dY"
      },
      "outputs": [],
      "source": [
        "def process_meta_features(data):\n",
        "    features_resource_ids = []\n",
        "\n",
        "    # Iterate through the features in data['Features']\n",
        "    for feature_name, feature_data in data['hasFeatures'].items():\n",
        "        # Add feature as resource\n",
        "        feature_resource = orkg.resources.add(label=feature_name).content\n",
        "        feature_resource_id = feature_resource['id']\n",
        "        features_resource_ids.append(feature_resource_id)\n",
        "\n",
        "        # Add feature data as properties\n",
        "        for meta_feature, value in feature_data.items():\n",
        "            # add Literal\n",
        "            literal_id = orkg.literals.add(label=str(value)).content['id']\n",
        "            # add the predicate\n",
        "            predicate_id = feature_predicates_2[meta_feature]\n",
        "\n",
        "            # connect them\n",
        "            statement_data = {\n",
        "                'subject_id': feature_resource_id,\n",
        "                'predicate_id': predicate_id,\n",
        "                'object_id': literal_id\n",
        "            }\n",
        "            orkg.statements.add(**statement_data)\n",
        "\n",
        "    return features_resource_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add the Meta features to the ORKG**"
      ],
      "metadata": {
        "id": "erx6hlbkSpZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF5wxCRsZSWa",
        "outputId": "fafd3fe5-6988-4211-8879-484478bbf05c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding kr-vs-kp\n",
            "Done kr-vs-kp\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:301: UserWarning: Multiple active versions of the dataset matching the name letter exist. Versions may be fundamentally different, returning version 1.\n",
            "  warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding letter\n",
            "Done letter\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:301: UserWarning: Multiple active versions of the dataset matching the name balance-scale exist. Versions may be fundamentally different, returning version 1.\n",
            "  warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding balance-scale\n",
            "Done balance-scale\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:301: UserWarning: Multiple active versions of the dataset matching the name mfeat-factors exist. Versions may be fundamentally different, returning version 1.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "for dataset in automl_datasets:\n",
        "    # Add dataset as a resource\n",
        "    resource = orkg.resources.add(label=f'{dataset} test', classes=[custom_classID]).content\n",
        "    resource_id = resource['id']\n",
        "\n",
        "    # Extract meta features for the current dataset\n",
        "    data = extract_meta_features(dataset)\n",
        "    time.sleep(1)\n",
        "    ids = process_meta_features(data)\n",
        "\n",
        "    # Construct dictionary of single value meta-features\n",
        "    single_value_metaFeatures = {\n",
        "        'Number of instances': data['Number of instances'],\n",
        "        'Number of features': data['Number of features'],\n",
        "        'Number of classes': data['Number of classes'],\n",
        "        'Standard Deviation Ratio': data['Standard Deviation Ratio'],\n",
        "        'Class Entropy': data['Class Entropy'],\n",
        "        'Normal Entropy': data['Normal Entropy'],\n",
        "        'hasFeature': ids\n",
        "    }\n",
        "\n",
        "    print(f\"Adding {dataset}\")\n",
        "    time.sleep(1)\n",
        "    # Iterate through single_value_metaFeatures\n",
        "    for feature, value in single_value_metaFeatures.items():\n",
        "        # Check if the feature is 'hasFeature'\n",
        "        if feature == 'hasFeature':\n",
        "            # Connect each feature resource ID to the dataset resource\n",
        "            for feature_resource_id in value:\n",
        "                statement_data = {\n",
        "                    'subject_id': resource_id,\n",
        "                    'predicate_id': hasFeatures_predicate_id,\n",
        "                    'object_id': feature_resource_id\n",
        "                }\n",
        "                orkg.statements.add(**statement_data)\n",
        "        else:\n",
        "            # Add Literal\n",
        "            literal_id = orkg.literals.add(label=str(value)).content['id']\n",
        "            # Add the predicate\n",
        "            predicate_id = feature_predicates[feature]\n",
        "\n",
        "            # Connect them\n",
        "            statement_data = {\n",
        "                'subject_id': resource_id,\n",
        "                'predicate_id': predicate_id,\n",
        "                'object_id': literal_id\n",
        "            }\n",
        "            orkg.statements.add(**statement_data)\n",
        "\n",
        "    print(f\"Done {dataset}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}